{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
=======
   "execution_count": 23,
>>>>>>> 22957a468786236d4518d6903bc3a500f6daa815
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "from utility import read_all_csvs_one_test\n",
    "from utility import run_cv_one_motor\n",
    "from utility import read_all_test_data_from_path\n",
    "from utility import read_all_test_data_from_path, show_reg_result,extract_selected_feature, prepare_sliding_window, FaultDetectReg\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_selection import f_regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and pre_processing"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": 24,
>>>>>>> 22957a468786236d4518d6903bc3a500f6daa815
   "metadata": {},
   "outputs": [],
   "source": [
    "n_int = 20\n",
    "\n",
    "# Subfunction for data preprocessing.\n",
    "def pre_processing(df: pd.DataFrame):\n",
    "    ''' ### Description\n",
    "    Preprocess the data:\n",
    "    - remove outliers\n",
    "    - add new features about the difference between the current and previous n data point.\n",
    "    '''\n",
    "    \n",
    "    # Function to design a Butterworth low-pass filter\n",
    "    def butter_lowpass(cutoff, fs, order=5):\n",
    "        nyquist = 0.5 * fs\n",
    "        normal_cutoff = cutoff / nyquist\n",
    "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "        return b, a\n",
    "\n",
    "\n",
    "    # Function to apply the Butterworth low-pass filter\n",
    "    def lowpass_filter(data, cutoff_freq, sampling_freq, order=5):\n",
    "        b, a = butter_lowpass(cutoff_freq, sampling_freq, order=order)\n",
    "        filtered_data = filtfilt(b, a, data)\n",
    "        return filtered_data\n",
    "\n",
    "\n",
    "    # Set parameters for the low-pass filter\n",
    "    cutoff_frequency = .8  # Adjust as needed\n",
    "    sampling_frequency = 10  # Assuming your data is evenly spaced in time\n",
    "\n",
    "\n",
    "    def customized_outlier_removal(df: pd.DataFrame):\n",
    "        ''' # Description\n",
    "        Remove outliers from the dataframe based on defined valid ranges. \n",
    "        Define a valid range of temperature and voltage. \n",
    "        Use ffil function to replace the invalid measurement with the previous value.\n",
    "        '''\n",
    "        df['position'] = df['position'].where(df['position'] <= 1000, np.nan)\n",
    "        df['position'] = df['position'].where(df['position'] >= 0, np.nan)\n",
    "        df['position'] = df['position'].ffill()\n",
    "        df['position'] = lowpass_filter(df['position'], cutoff_frequency, sampling_frequency)\n",
    "        df['position'] = df['position'].rolling(window=20, min_periods=1).mean()\n",
    "        df['position'] = df['position'].round()\n",
    "\n",
    "        df['temperature'] = df['temperature'].where(df['temperature'] <= 100, np.nan)\n",
    "        df['temperature'] = df['temperature'].where(df['temperature'] >= 0, np.nan)\n",
    "        df['temperature'] = df['temperature'].rolling(window=20, min_periods=1).mean()\n",
    "\n",
    "        # Make sure that the difference between the current and previous temperature cannot be too large.\n",
    "        # Define your threshold\n",
    "        threshold = 5\n",
    "        # Shift the 'temperature' column by one row to get the previous temperature\n",
    "        prev_tmp = df['temperature'].shift(1)\n",
    "        # Calculate the absolute difference between current and previous temperature\n",
    "        temp_diff = np.abs(df['temperature'] - prev_tmp)\n",
    "        # Set the temperature to NaN where the difference is larger than the threshold\n",
    "        df.loc[temp_diff > threshold, 'temperature'] = np.nan\n",
    "        df['temperature'] = df['temperature'].ffill()\n",
    "\n",
    "        df['voltage'] = df['voltage'].where(df['voltage'] >= 6000, np.nan)\n",
    "        df['voltage'] = df['voltage'].where(df['voltage'] <= 8000, np.nan)\n",
    "        df['voltage'] = df['voltage'].ffill()\n",
    "        df['voltage'] = lowpass_filter(df['voltage'], cutoff_frequency, sampling_frequency)\n",
    "        df['voltage'] = df['voltage'].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "\n",
    "    def cal_diff(df: pd.DataFrame, n_int: int):\n",
    "        ''' # Description\n",
    "        Calculate the difference between the current and previous n data point.\n",
    "        '''\n",
    "        # Tranform the features relative to the first data point.\n",
    "        df['temperature'] = df['temperature'] - df['temperature'].iloc[0]\n",
    "        df['voltage'] = df['voltage'] - df['voltage'].iloc[0]\n",
    "        df['position'] = df['position'] - df['position'].iloc[0]\n",
    "\n",
    "        # Calculate the difference between the current and previous n data point.\n",
    "        df['temperature_diff'] = df['temperature'].diff(n_int)\n",
    "        df['voltage_diff'] = df['voltage'].diff(n_int)\n",
    "        df['position_diff'] = df['position'].diff(n_int)   \n",
    "\n",
    "    # Start processing.\n",
    "    customized_outlier_removal(df)\n",
    "    cal_diff(df, n_int)\n",
    "\n",
    "base_dictionary = '../../dataset/training_data/'\n",
    "df_data = read_all_test_data_from_path(base_dictionary, pre_processing, is_plot=False)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the folders in the base_dictionary\n",
    "path_list = os.listdir(base_dictionary)\n",
    "# Only keep the folders, not the excel file.\n",
    "path_list = path_list[:-1]\n",
    "\n",
    "# Read the data.\n",
    "df_data_smoothing = pd.DataFrame()\n",
    "for tmp_path in path_list:\n",
    "    path = base_dictionary + tmp_path\n",
    "    # Read the data with the customized outlier removal function.\n",
    "    tmp_df = read_all_csvs_one_test(path, tmp_path, pre_processing)\n",
    "    df_data_smoothing = pd.concat([df_data_smoothing, tmp_df])\n",
    "    df_data_smoothing = df_data_smoothing.reset_index(drop=True)\n",
    "\n",
    "# Read the test conditions\n",
    "df_test_conditions = pd.read_excel(base_dictionary+'Test conditions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_test_id = ['20240105_164214', \n",
    "    '20240105_165300', \n",
    "    '20240105_165972', \n",
    "    '20240320_152031', \n",
    "    '20240320_153841', \n",
    "    '20240320_155664', \n",
    "    '20240321_122650', \n",
    "    '20240325_135213', \n",
    "    '20240426_141190', \n",
    "    '20240426_141532', \n",
    "    '20240426_141602', \n",
    "    '20240426_141726', \n",
    "    '20240426_141938', \n",
    "    '20240426_141980', \n",
    "    '20240503_164435']\n",
    "df_data = df_data[df_data['test_condition'].isin(normal_test_id)]\n",
    "df_data_smoothing = df_data_smoothing[df_data_smoothing['test_condition'].isin(normal_test_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Steps for Linear Regression\n",
    "linear_regression_steps = [\n",
    "    ('Normalizer', MinMaxScaler()),  # Step 1: Normalization\n",
    "    #('Standardizer', StandardScaler),  # Step 1: Standardization\n",
    "    ('regressor', LinearRegression())    # Step 2: Linear Regression\n",
    "]\n",
    "\n",
    "# Steps for Ridge Regression\n",
    "ridge_regression_steps = [\n",
    "    ('Normalizer', MinMaxScaler()),  # Step 1: Normalization\n",
    "    #('Standardizer', StandardScaler),  # Step 1: Standardization\n",
    "    ('regressor', Ridge())               # Step 2: Ridge Regression\n",
    "]\n",
    "\n",
    "# Steps for Lasso Regression\n",
    "lasso_regression_steps = [\n",
    "    ('Normalizer', MinMaxScaler()),  # Step 1: Normalization\n",
    "    #('Standardizer', StandardScaler),  # Step 1: Standardizatio\n",
    "    ('regressor', Lasso())               # Step 2: Lasso Regression\n",
    "]\n",
    "\n",
    "# Steps for ElasticNet Regression\n",
    "elasticnet_regression_steps = [\n",
    "    ('Normalizer', MinMaxScaler()),  # Step 1: Normalization\n",
    "    #('Standardizer', StandardScaler),  # Step 1: Standardization\n",
    "    ('regressor', ElasticNet())          # Step 2: ElasticNet Regression\n",
    "]\n",
    "\n",
    "# Steps for Decision Tree Regression\n",
    "decision_tree_steps = [\n",
    "    ('regressor', DecisionTreeRegressor())  # Step 2: Decision Tree Regressor\n",
    "]\n",
    "\n",
    "# Initialize Pipelines for each model\n",
    "mdl_linear_regression = Pipeline(linear_regression_steps)\n",
    "mdl_ridge_regression = Pipeline(ridge_regression_steps)\n",
    "mdl_lasso_regression = Pipeline(lasso_regression_steps)\n",
    "mdl_elasticnet_regression = Pipeline(elasticnet_regression_steps)\n",
    "mdl_decision_tree = Pipeline(decision_tree_steps)\n",
    "\n",
    "# List of models to be used in GridSearchCV\n",
    "models = [\n",
    "    ('Linear Regression', mdl_linear_regression),\n",
    "    ('Ridge Regression', mdl_ridge_regression),\n",
    "    ('Lasso Regression', mdl_lasso_regression),\n",
    "    ('ElasticNet Regression', mdl_elasticnet_regression),\n",
    "    ('Decision Tree Regression', mdl_decision_tree),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import run_cv_one_motor\n",
    "\n",
    "\n",
    "def run_all_motors(df_data, mdl, feature_list, n_fold=5, threshold=3, window_size=1, sample_step=1,\n",
    "            prediction_lead_time=1, single_run_result=True, mdl_type='reg'):\n",
    "    all_results = []\n",
    "    # Loop over all the six motors.\n",
    "    for i in range(1, 7):\n",
    "        print(f'Model for predicting temperature of motor {i}:')\n",
    "        # Run cross validation.\n",
    "        df_perf = run_cv_one_motor(motor_idx=i, df_data=df_data, mdl=mdl, feature_list=feature_list,\n",
    "                n_fold=n_fold, threshold=threshold, window_size=window_size, sample_step=sample_step,\n",
    "            prediction_lead_time=prediction_lead_time, single_run_result=single_run_result, mdl_type=mdl_type)\n",
    "        all_results.append(df_perf)\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models_motor6(df_data, models, feature_list, n_fold=5, threshold=3, window_size=1, sample_step=1,\n",
    "                   prediction_lead_time=1, single_run_result=True, mdl_type='reg'):\n",
    "    summary_results = []\n",
    "    for name, mdl in models:\n",
    "        print(f'Running model: {name}')\n",
    "        df_perf  = run_cv_one_motor(motor_idx=6, df_data=df_data, mdl=mdl, feature_list=feature_list,\n",
    "                n_fold=n_fold, threshold=threshold, window_size=window_size, sample_step=sample_step,\n",
    "            prediction_lead_time=prediction_lead_time, single_run_result=single_run_result, mdl_type=mdl_type)\n",
    "        \n",
    "        summary_results.append((name, df_perf))\n",
    "    \n",
    "    return summary_results\n",
    "\n",
    "\n",
    "def run_all_models(df_data, models, feature_list, n_fold=5, threshold=3, window_size=1, sample_step=1,\n",
    "                   prediction_lead_time=1, single_run_result=True, mdl_type='reg'):\n",
    "    summary_results = []\n",
    "    for name, mdl in models:\n",
    "        print(f'Running model: {name}')\n",
    "        all_results = run_all_motors(df_data, mdl, feature_list, n_fold=n_fold, threshold=threshold, \n",
    "                                     window_size=window_size, sample_step=sample_step,\n",
    "                                     prediction_lead_time=prediction_lead_time, single_run_result=single_run_result, \n",
    "                                     mdl_type=mdl_type)\n",
    "        \n",
    "        df_all_results = pd.concat(all_results, keys=[f'Motor_{i}' for i in range(1, 7)])\n",
    "        summary_results.append((name, df_all_results))\n",
    "    \n",
    "    return summary_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: Linear Regression\n",
      "Model for motor 6:\n",
      "   Max error      RMSE  Exceed boundary rate\n",
      "0  13.693682  4.926842              0.843298\n",
      "1   6.438077  2.220468              0.674858\n",
      "2  14.082472  7.054126              0.874207\n",
      "3   5.928427  2.883518              0.642473\n",
      "4   9.472806  5.735191              1.000000\n",
      "\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "Max error: 9.9231 +- 3.8673\n",
      "RMSE: 4.5640 +- 2.0013\n",
      "Exceed boundary rate: 0.8070 +- 0.1480\n",
      "\n",
      "\n",
      "Running model: Ridge Regression\n",
      "Model for motor 6:\n",
      "   Max error      RMSE  Exceed boundary rate\n",
      "0  13.564431  4.859087              0.839401\n",
      "1   6.337341  2.206610              0.673913\n",
      "2  13.918247  6.962545              0.874207\n",
      "3   5.869100  2.855020              0.641129\n",
      "4   9.472488  5.738678              1.000000\n",
      "\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "Max error: 9.8323 +- 3.8300\n",
      "RMSE: 4.5244 +- 1.9805\n",
      "Exceed boundary rate: 0.8057 +- 0.1484\n",
      "\n",
      "\n",
      "Running model: Lasso Regression\n",
      "Model for motor 6:\n",
      "   Max error      RMSE  Exceed boundary rate\n",
      "0   6.358686  1.139367              0.113117\n",
      "1   2.785689  0.841022              0.114367\n",
      "2   8.998435  3.407160              0.748184\n",
      "3   3.635089  1.367069              0.443548\n",
      "4   7.457042  4.002409              0.814530\n",
      "\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "Max error: 5.8470 +- 2.6005\n",
      "RMSE: 2.1514 +- 1.4457\n",
      "Exceed boundary rate: 0.4467 +- 0.3346\n",
      "\n",
      "\n",
      "Running model: ElasticNet Regression\n",
      "Model for motor 6:\n",
      "   Max error      RMSE  Exceed boundary rate\n",
      "0   6.358686  1.139367              0.113117\n",
      "1   2.785689  0.841022              0.114367\n",
      "2   8.998435  3.407160              0.748184\n",
      "3   3.635089  1.367069              0.443548\n",
      "4   7.457042  4.002409              0.814530\n",
      "\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "Max error: 5.8470 +- 2.6005\n",
      "RMSE: 2.1514 +- 1.4457\n",
      "Exceed boundary rate: 0.4467 +- 0.3346\n",
      "\n",
      "\n",
      "Running model: Decision Tree Regression\n",
      "Model for motor 6:\n",
      "   Max error      RMSE  Exceed boundary rate\n",
      "0       5.00  2.485940              0.652651\n",
      "1       5.85  2.525250              0.318526\n",
      "2       8.65  3.193176              0.726969\n",
      "3       3.80  1.239914              0.211022\n",
      "4       9.10  4.749686              0.795038\n",
      "\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "Max error: 6.4800 +- 2.3099\n",
      "RMSE: 2.8388 +- 1.2803\n",
      "Exceed boundary rate: 0.5408 +- 0.2598\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_list_all = ['time', 'data_motor_1_position', 'data_motor_1_temperature','data_motor_1_voltage',\n",
    "       'data_motor_1_temperature_diff', 'data_motor_1_voltage_diff','data_motor_1_position_diff', \n",
    "       'data_motor_2_position','data_motor_2_temperature', 'data_motor_2_voltage', \n",
    "       'data_motor_2_temperature_diff', 'data_motor_2_voltage_diff', 'data_motor_2_position_diff',\n",
    "       'data_motor_3_position', 'data_motor_3_temperature','data_motor_3_voltage',\n",
    "       'data_motor_3_temperature_diff', 'data_motor_3_voltage_diff','data_motor_3_position_diff', \n",
    "       'data_motor_4_position','data_motor_4_temperature', 'data_motor_4_voltage',\n",
    "       'data_motor_4_temperature_diff', 'data_motor_4_voltage_diff', 'data_motor_4_position_diff',\n",
    "       'data_motor_5_position', 'data_motor_5_temperature','data_motor_5_voltage',\n",
    "       'data_motor_5_temperature_diff', 'data_motor_5_voltage_diff','data_motor_5_position_diff', \n",
    "       'data_motor_6_position', 'data_motor_6_temperature', 'data_motor_6_voltage',\n",
    "       'data_motor_6_temperature_diff', 'data_motor_6_voltage_diff', 'data_motor_6_position_diff']\n",
    "\n",
    "\n",
    "selected_features= ['time','data_motor_1_position',  'data_motor_1_temperature', \n",
    "                    'data_motor_2_position',  \n",
    "                    'data_motor_3_position',  \n",
    "                    'data_motor_4_position', 'data_motor_4_temperature',\n",
    "                    'data_motor_5_position', 'data_motor_5_temperature',\n",
    "                    'data_motor_6_position', 'data_motor_6_temperature', \n",
    "                    'data_motor_1_voltage']\n",
    "\n",
    "threshold = 1\n",
    "window_size = 1\n",
    "sample_step = 1\n",
    "prediction_lead_time = 1 # We add the temperature measurement up to 1 point before the current time.\n",
    "\n",
    "#all_model_results = run_all_models(df_data=df_data_smoothing, models=models, feature_list=selected_features, threshold=threshold, window_size=window_size, sample_step=sample_step, prediction_lead_time=prediction_lead_time, single_run_result=False)\n",
    "\n",
    "all_model_results = run_all_models_motor6(df_data=df_data_smoothing, models=models, feature_list=selected_features, threshold=threshold, window_size=window_size, sample_step=sample_step, prediction_lead_time=prediction_lead_time, single_run_result=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the results - Only using features from the current moment\n",
      "\n",
      "| Model                   | Max error | RMSE  | Exceed boundary rate |\n",
      "|-------------------------|-----------|-------|----------------------|\n",
      "| Linear Regression        |     9.92 |   4.56 |   0.81 |\n",
      "| Ridge Regression         |     9.83 |   4.52 |   0.81 |\n",
      "| Lasso Regression         |     5.85 |   2.15 |   0.45 |\n",
      "| ElasticNet Regression    |     5.85 |   2.15 |   0.45 |\n",
      "| Decision Tree Regression |     6.48 |   2.84 |   0.54 |\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reduction operation 'argmin' not allowed for this dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m     exceed_boundary_rate \u001b[38;5;241m=\u001b[39m df_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExceed boundary rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;241m.\u001b[39mljust(\u001b[38;5;241m24\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_error\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m8.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexceed_boundary_rate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m best_model \u001b[38;5;241m=\u001b[39m summary_df\u001b[38;5;241m.\u001b[39mloc[\u001b[43msummary_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRMSE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midxmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe best model is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with a Mean Squared Error (RMSE) of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:2495\u001b[0m, in \u001b[0;36mSeries.idxmin\u001b[1;34m(self, axis, skipna, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21midxmin\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, skipna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Hashable:\n\u001b[0;32m   2433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2434\u001b[0m \u001b[38;5;124;03m    Return the row label of the minimum value.\u001b[39;00m\n\u001b[0;32m   2435\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;124;03m    nan\u001b[39;00m\n\u001b[0;32m   2494\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2495\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2497\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\base.py:717\u001b[0m, in \u001b[0;36mIndexOpsMixin.argmin\u001b[1;34m(self, axis, skipna, *args, **kwargs)\u001b[0m\n\u001b[0;32m    713\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m delegate\u001b[38;5;241m.\u001b[39margmin()\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[int, ndarray]\", expected\u001b[39;00m\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;66;03m# \"int\")\u001b[39;00m\n\u001b[1;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanargmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m    718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\nanops.py:88\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck(obj) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m obj_iter):\n\u001b[0;32m     87\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreduction operation \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not allowed for this dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     )\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: reduction operation 'argmin' not allowed for this dtype"
     ]
    }
   ],
   "source": [
    "summary_data = {\n",
    "    'Model': [],\n",
    "    'Max error': [],\n",
    "    'RMSE': [],\n",
    "    'Exceed boundary rate': []\n",
    "}\n",
    "\n",
    "for model_name, df_results in all_model_results:\n",
    "    max_error = df_results['Max error'].mean()\n",
    "    mse = df_results['RMSE'].mean()\n",
    "    exceed_boundary_rate = df_results['Exceed boundary rate'].mean()\n",
    "    \n",
    "    summary_data['Model'].append(model_name)\n",
    "    summary_data['Max error'].append(f'{max_error:.2f}')\n",
    "    summary_data['RMSE'].append(f'{mse:.2f}')\n",
    "    summary_data['Exceed boundary rate'].append(f'{exceed_boundary_rate:.2f}')\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"Summary of the results - Only using features from the current moment\\n\")\n",
    "print(\"| Model                   | Max error | RMSE  | Exceed boundary rate |\")\n",
    "print(\"|-------------------------|-----------|-------|----------------------|\")\n",
    "\n",
    "for model_name, df_results in all_model_results:\n",
    "    max_error = df_results['Max error'].mean()\n",
    "    mse = df_results['RMSE'].mean()\n",
    "    exceed_boundary_rate = df_results['Exceed boundary rate'].mean()\n",
    "    \n",
    "    print(f\"| {model_name.ljust(24)} | {max_error:8.2f} | {mse:6.2f} | {exceed_boundary_rate:6.2f} |\")\n",
    "\n",
    "best_model = summary_df.loc[summary_df['RMSE'].idxmin()]\n",
    "print(f\"\\nThe best model is {best_model['Model']} with a Mean Squared Error (RMSE) of {best_model['RMSE']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of the results - Only using features from the current moment**\n",
    "\n",
    "Please add a table in the end, summarying the results from all the models. Please write a few texts to explain what is the best model you got (including the features and preprocessing you did), its performance, and how could you further improve it.\n",
    "\n",
    "| Model                   | Max error | RMSE  | Exceed boundary rate |\n",
    "|-------------------------|-----------|-------|----------------------|\n",
    "| Linear Regression        |     9.92 |   4.56 |   0.49 |\n",
    "| Ridge Regression         |     9.83 |   4.52 |   0.49 |\n",
    "| Lasso Regression         |     5.85 |   2.15 |   0.23 |\n",
    "| ElasticNet Regression    |     5.85 |   2.15 |   0.23 |\n",
    "| Decision Tree Regression |     6.85 |   2.48 |   0.21 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 2: Include the features in the past\n",
    "\n",
    "Now, let's consider using the sliding window approach to include the past in the regression model as well. Please have a look at the demo notebook, run your experiment, and report the best models you could have if you apply the sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: Linear Regression\n",
      "Model for motor 6:\n",
      "   Max error      RMSE  Exceed boundary rate\n",
      "0   0.079289  0.009912                   0.0\n",
      "1   0.052445  0.006247                   0.0\n",
      "2   0.068870  0.007215                   0.0\n",
      "3   0.053624  0.009681                   0.0\n",
      "4   0.051144  0.009094                   0.0\n",
      "\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "Max error: 0.0611 +- 0.0125\n",
      "RMSE: 0.0084 +- 0.0016\n",
      "Exceed boundary rate: 0.0000 +- 0.0000\n",
      "\n",
      "\n",
      "Running model: Ridge Regression\n",
      "Model for motor 6:\n",
      "   Max error      RMSE  Exceed boundary rate\n",
      "0   0.276469  0.032161                   0.0\n",
      "1   0.227679  0.047462                   0.0\n",
      "2   0.297830  0.040667                   0.0\n",
      "3   0.274548  0.084597                   0.0\n",
      "4   0.242037  0.061410                   0.0\n",
      "\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "Max error: 0.2637 +- 0.0283\n",
      "RMSE: 0.0533 +- 0.0205\n",
      "Exceed boundary rate: 0.0000 +- 0.0000\n",
      "\n",
      "\n",
      "Running model: Lasso Regression\n",
      "Model for motor 6:\n",
      "   Max error      RMSE  Exceed boundary rate\n",
      "0   6.348762  1.145602              0.114848\n",
      "1   2.777140  0.896981              0.132821\n",
      "2   8.994625  3.435741              0.761084\n",
      "3   3.626586  1.517928              0.552764\n",
      "4   7.472153  4.191499              0.860285\n",
      "\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "Max error: 5.8439 +- 2.6055\n",
      "RMSE: 2.2376 +- 1.4799\n",
      "Exceed boundary rate: 0.4844 +- 0.3474\n",
      "\n",
      "\n",
      "Running model: ElasticNet Regression\n",
      "Model for motor 6:\n",
      "   Max error      RMSE  Exceed boundary rate\n",
      "0   5.298294  0.924395              0.069867\n",
      "1   2.777140  0.896981              0.132821\n",
      "2   8.994625  3.435741              0.761084\n",
      "3   3.626586  1.517928              0.552764\n",
      "4   7.472153  4.191499              0.860285\n",
      "\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "Max error: 5.6338 +- 2.5970\n",
      "RMSE: 2.1933 +- 1.5234\n",
      "Exceed boundary rate: 0.4754 +- 0.3597\n",
      "\n",
      "\n",
      "Running model: Decision Tree Regression\n",
      "Model for motor 6:\n",
      "   Max error      RMSE  Exceed boundary rate\n",
      "0       0.70  0.059883              0.000000\n",
      "1       0.15  0.019319              0.000000\n",
      "2       2.70  0.355189              0.032372\n",
      "3       0.25  0.039380              0.000000\n",
      "4       4.40  1.668929              0.466365\n",
      "\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "Max error: 1.6400 +- 1.8559\n",
      "RMSE: 0.4285 +- 0.7069\n",
      "Exceed boundary rate: 0.0997 +- 0.2054\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 1\n",
    "window_size = 50\n",
    "sample_step = 1\n",
    "prediction_lead_time = 1 # We add the temperature measurement up to 1 point before the current time.\n",
    "\n",
    "#all_model_results = run_all_models(df_data=df_data_smoothing, models=models, feature_list=selected_features, threshold=threshold, window_size=window_size, sample_step=sample_step, prediction_lead_time=prediction_lead_time, single_run_result=False)\n",
    "\n",
    "all_model_results = run_all_models_motor6(df_data=df_data_smoothing, models=models, feature_list=selected_features, threshold=threshold, window_size=window_size, sample_step=sample_step, prediction_lead_time=prediction_lead_time, single_run_result=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the results - Only using features from the current moment\n",
      "\n",
      "| Model                   | Max error | RMSE  | Exceed boundary rate |\n",
      "|-------------------------|-----------|-------|----------------------|\n",
      "| Linear Regression        |     0.06 |   0.01 |   0.00 |\n",
      "| Ridge Regression         |     0.26 |   0.05 |   0.00 |\n",
      "| Lasso Regression         |     5.84 |   2.24 |   0.48 |\n",
      "| ElasticNet Regression    |     5.63 |   2.19 |   0.48 |\n",
      "| Decision Tree Regression |     1.64 |   0.43 |   0.10 |\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reduction operation 'argmin' not allowed for this dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m     exceed_boundary_rate \u001b[38;5;241m=\u001b[39m df_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExceed boundary rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;241m.\u001b[39mljust(\u001b[38;5;241m24\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_error\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m8.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexceed_boundary_rate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m best_model \u001b[38;5;241m=\u001b[39m summary_df\u001b[38;5;241m.\u001b[39mloc[\u001b[43msummary_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRMSE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midxmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe best model is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with a Mean Squared Error (RMSE) of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:2495\u001b[0m, in \u001b[0;36mSeries.idxmin\u001b[1;34m(self, axis, skipna, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21midxmin\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, skipna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Hashable:\n\u001b[0;32m   2433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2434\u001b[0m \u001b[38;5;124;03m    Return the row label of the minimum value.\u001b[39;00m\n\u001b[0;32m   2435\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;124;03m    nan\u001b[39;00m\n\u001b[0;32m   2494\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2495\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2497\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\base.py:717\u001b[0m, in \u001b[0;36mIndexOpsMixin.argmin\u001b[1;34m(self, axis, skipna, *args, **kwargs)\u001b[0m\n\u001b[0;32m    713\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m delegate\u001b[38;5;241m.\u001b[39margmin()\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[int, ndarray]\", expected\u001b[39;00m\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;66;03m# \"int\")\u001b[39;00m\n\u001b[1;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanargmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m    718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\nanops.py:88\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck(obj) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m obj_iter):\n\u001b[0;32m     87\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreduction operation \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not allowed for this dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     )\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: reduction operation 'argmin' not allowed for this dtype"
     ]
    }
   ],
   "source": [
    "summary_data = {\n",
    "    'Model': [],\n",
    "    'Max error': [],\n",
    "    'RMSE': [],\n",
    "    'Exceed boundary rate': []\n",
    "}\n",
    "\n",
    "for model_name, df_results in all_model_results:\n",
    "    max_error = df_results['Max error'].mean()\n",
    "    mse = df_results['RMSE'].mean()\n",
    "    exceed_boundary_rate = df_results['Exceed boundary rate'].mean()\n",
    "    \n",
    "    summary_data['Model'].append(model_name)\n",
    "    summary_data['Max error'].append(f'{max_error:.2f}')\n",
    "    summary_data['RMSE'].append(f'{mse:.2f}')\n",
    "    summary_data['Exceed boundary rate'].append(f'{exceed_boundary_rate:.2f}')\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"Summary of the results - Only using features from the current moment\\n\")\n",
    "print(\"| Model                   | Max error | RMSE  | Exceed boundary rate |\")\n",
    "print(\"|-------------------------|-----------|-------|----------------------|\")\n",
    "\n",
    "for model_name, df_results in all_model_results:\n",
    "    max_error = df_results['Max error'].mean()\n",
    "    mse = df_results['RMSE'].mean()\n",
    "    exceed_boundary_rate = df_results['Exceed boundary rate'].mean()\n",
    "    \n",
    "    print(f\"| {model_name.ljust(24)} | {max_error:8.2f} | {mse:6.2f} | {exceed_boundary_rate:6.2f} |\")\n",
    "\n",
    "best_model = summary_df.loc[summary_df['RMSE'].idxmin()]\n",
    "print(f\"\\nThe best model is {best_model['Model']} with a Mean Squared Error (RMSE) of {best_model['RMSE']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of the results - Sliding window**\n",
    "\n",
    "Please add a table in the end, summarying the results from all the models. Please write a few texts to explain what is the best model you got (including the features and preprocessing you did), its performance, and how could you further improve it.\n",
    "\n",
    "| Model   (threshold = 1; window_size = 50 ;sample_step = 1 ; prediction_lead_time = 1) | Max error | MRSE | Exceed boundary rate |\n",
    "|---------|----------|-----------|--------|\n",
    "| Linear Regression        |     0.06 |   0.01 |   0.00 |\n",
    "| Ridge Regression         |     0.26 |   0.05 |   0.00 |\n",
    "| Lasso Regression         |     5.84 |   2.24 |   0.48 |\n",
    "| ElasticNet Regression    |     5.63 |   2.19 |   0.48 |\n",
    "| Decision Tree Regression |     1.67 |   0.44 |   0.10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Fault detection based on regression model\n",
    "\n",
    "In this exercise, we use the dataset that contains the failure of motor 6 to test the fault detection model based on the regression model trained before. \n",
    "\n",
    "[This notebook](demo_FaultDetectReg.ipynb) presents a demonstration of how to use the provided supporting function to develop fault detection model based on the regression model. Please have a look at this notebook, and try to improve the performance of the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_int = 20\n",
    "\n",
    "# Subfunction for data preprocessing.\n",
    "def pre_processing(df: pd.DataFrame):\n",
    "    ''' ### Description\n",
    "    Preprocess the data:\n",
    "    - remove outliers\n",
    "    - add new features about the difference between the current and previous n data point.\n",
    "    '''\n",
    "    \n",
    "    # Function to design a Butterworth low-pass filter\n",
    "    def butter_lowpass(cutoff, fs, order=5):\n",
    "        nyquist = 0.5 * fs\n",
    "        normal_cutoff = cutoff / nyquist\n",
    "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "        return b, a\n",
    "\n",
    "\n",
    "    # Function to apply the Butterworth low-pass filter\n",
    "    def lowpass_filter(data, cutoff_freq, sampling_freq, order=5):\n",
    "        b, a = butter_lowpass(cutoff_freq, sampling_freq, order=order)\n",
    "        filtered_data = filtfilt(b, a, data)\n",
    "        return filtered_data\n",
    "\n",
    "\n",
    "    # Set parameters for the low-pass filter\n",
    "    cutoff_frequency = .8  # Adjust as needed\n",
    "    sampling_frequency = 10  # Assuming your data is evenly spaced in time\n",
    "\n",
    "\n",
    "    def customized_outlier_removal(df: pd.DataFrame):\n",
    "        ''' # Description\n",
    "        Remove outliers from the dataframe based on defined valid ranges. \n",
    "        Define a valid range of temperature and voltage. \n",
    "        Use ffil function to replace the invalid measurement with the previous value.\n",
    "        '''\n",
    "        df['position'] = df['position'].where(df['position'] <= 1000, np.nan)\n",
    "        df['position'] = df['position'].where(df['position'] >= 0, np.nan)\n",
    "        df['position'] = df['position'].ffill()\n",
    "        df['position'] = lowpass_filter(df['position'], cutoff_frequency, sampling_frequency)\n",
    "        df['position'] = df['position'].rolling(window=20, min_periods=1).mean()\n",
    "        df['position'] = df['position'].round()\n",
    "\n",
    "        df['temperature'] = df['temperature'].where(df['temperature'] <= 100, np.nan)\n",
    "        df['temperature'] = df['temperature'].where(df['temperature'] >= 0, np.nan)\n",
    "        df['temperature'] = df['temperature'].rolling(window=20, min_periods=1).mean()\n",
    "\n",
    "        # Make sure that the difference between the current and previous temperature cannot be too large.\n",
    "        # Define your threshold\n",
    "        threshold = 5\n",
    "        # Shift the 'temperature' column by one row to get the previous temperature\n",
    "        prev_tmp = df['temperature'].shift(1)\n",
    "        # Calculate the absolute difference between current and previous temperature\n",
    "        temp_diff = np.abs(df['temperature'] - prev_tmp)\n",
    "        # Set the temperature to NaN where the difference is larger than the threshold\n",
    "        df.loc[temp_diff > threshold, 'temperature'] = np.nan\n",
    "        df['temperature'] = df['temperature'].ffill()\n",
    "\n",
    "        df['voltage'] = df['voltage'].where(df['voltage'] >= 6000, np.nan)\n",
    "        df['voltage'] = df['voltage'].where(df['voltage'] <= 8000, np.nan)\n",
    "        df['voltage'] = df['voltage'].ffill()\n",
    "        df['voltage'] = lowpass_filter(df['voltage'], cutoff_frequency, sampling_frequency)\n",
    "        df['voltage'] = df['voltage'].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "\n",
    "    def cal_diff(df: pd.DataFrame, n_int: int):\n",
    "        ''' # Description\n",
    "        Calculate the difference between the current and previous n data point.\n",
    "        '''\n",
    "        # Tranform the features relative to the first data point.\n",
    "        df['temperature'] = df['temperature'] - df['temperature'].iloc[0]\n",
    "        df['voltage'] = df['voltage'] - df['voltage'].iloc[0]\n",
    "        df['position'] = df['position'] - df['position'].iloc[0]\n",
    "\n",
    "        # Calculate the difference between the current and previous n data point.\n",
    "        df['temperature_diff'] = df['temperature'].diff(n_int)\n",
    "        df['voltage_diff'] = df['voltage'].diff(n_int)\n",
    "        df['position_diff'] = df['position'].diff(n_int)   \n",
    "\n",
    "    # Start processing.\n",
    "    customized_outlier_removal(df)\n",
    "    cal_diff(df, n_int)\n",
    "\n",
    "base_dictionary = '../../dataset/testing_data/'\n",
    "df_data = read_all_test_data_from_path(base_dictionary, pre_processing, is_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the folders in the base_dictionary\n",
    "path_list = os.listdir(base_dictionary)\n",
    "# Only keep the folders, not the excel file.\n",
    "path_list = path_list[:-1]\n",
    "\n",
    "# Read the data.\n",
    "df_data_smoothing = pd.DataFrame()\n",
    "for tmp_path in path_list:\n",
    "    path = base_dictionary + tmp_path\n",
    "    # Read the data with the customized outlier removal function.\n",
    "    tmp_df = read_all_csvs_one_test(path, tmp_path, pre_processing)\n",
    "    df_data_smoothing = pd.concat([df_data_smoothing, tmp_df])\n",
    "    df_data_smoothing = df_data_smoothing.reset_index(drop=True)\n",
    "\n",
    "# Read the test conditions\n",
    "df_test_conditions = pd.read_excel(base_dictionary+'Test conditions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Steps for Linear Regression\n",
    "linear_regression_steps = [\n",
    "    ('Normalizer', MinMaxScaler()),  # Step 1: Normalization\n",
    "    #('Standardizer', StandardScaler),  # Step 1: Standardization\n",
    "    ('regressor', LinearRegression())    # Step 2: Linear Regression\n",
    "]\n",
    "\n",
    "# Steps for Ridge Regression\n",
    "ridge_regression_steps = [\n",
    "    ('Normalizer', MinMaxScaler()),  # Step 1: Normalization\n",
    "    #('Standardizer', StandardScaler),  # Step 1: Standardization\n",
    "    ('regressor', Ridge())               # Step 2: Ridge Regression\n",
    "]\n",
    "\n",
    "# Steps for Lasso Regression\n",
    "lasso_regression_steps = [\n",
    "    ('Normalizer', MinMaxScaler()),  # Step 1: Normalization\n",
    "    #('Standardizer', StandardScaler),  # Step 1: Standardizatio\n",
    "    ('regressor', Lasso())               # Step 2: Lasso Regression\n",
    "]\n",
    "\n",
    "# Steps for ElasticNet Regression\n",
    "elasticnet_regression_steps = [\n",
    "    ('Normalizer', MinMaxScaler()),  # Step 1: Normalization\n",
    "    #('Standardizer', StandardScaler),  # Step 1: Standardization\n",
    "    ('regressor', ElasticNet())          # Step 2: ElasticNet Regression\n",
    "]\n",
    "\n",
    "# Steps for Decision Tree Regression\n",
    "decision_tree_steps = [\n",
    "    ('regressor', DecisionTreeRegressor())  # Step 2: Decision Tree Regressor\n",
    "]\n",
    "\n",
    "# Initialize Pipelines for each model\n",
    "mdl_linear_regression = Pipeline(linear_regression_steps)\n",
    "mdl_ridge_regression = Pipeline(ridge_regression_steps)\n",
    "mdl_lasso_regression = Pipeline(lasso_regression_steps)\n",
    "mdl_elasticnet_regression = Pipeline(elasticnet_regression_steps)\n",
    "mdl_decision_tree = Pipeline(decision_tree_steps)\n",
    "\n",
    "# List of models to be used in GridSearchCV\n",
    "models = [\n",
    "    ('Linear Regression', mdl_linear_regression),\n",
    "    ('Ridge Regression', mdl_ridge_regression),\n",
    "    ('Lasso Regression', mdl_lasso_regression),\n",
    "    ('ElasticNet Regression', mdl_elasticnet_regression),\n",
    "    ('Decision Tree Regression', mdl_decision_tree),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
>>>>>>> 22957a468786236d4518d6903bc3a500f6daa815
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train the model.\n",
    "# Get all the normal data.\n",
    "normal_test_id = ['20240527_094865',\n",
    "                  '20240527_101627',\n",
    "                  '20240527_101627',\n",
    "                  '20240527_102436',\n",
    "                  '20240527_102919',\n",
    "                  '20240527_103311',\n",
    "                  '20240527_103690',\n",
    "                  '20240527_104247']\n",
    "\n",
<<<<<<< HEAD
    "df_tr = df_data[df_data['test_condition'].isin(normal_test_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = [\n",
    "    '20240325_155003',\n",
    "    '20240425_093699',\n",
    "    '20240425_094425',\n",
    "    '20240426_140055',\n",
    "    '20240503_163963',\n",
    "    '20240503_164675',\n",
    "    '20240503_165189'\n",
    "]\n",
    "df_test = df_data[df_data['test_condition'].isin(test_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_all_0 = ['time',\n",
    "                 'data_motor_2_position', \n",
    "                'data_motor_3_position', \n",
    "                'data_motor_4_position', 'data_motor_3_temperature',\n",
    "                  'data_motor_6_temperature']\n",
=======
    "df_tr = df_data[df_data['test_condition'].isin(normal_test_id)]\n",
>>>>>>> 22957a468786236d4518d6903bc3a500f6daa815
    "\n",
    "feature_list_all = ['time', 'data_motor_1_position', 'data_motor_1_temperature','data_motor_1_voltage',\n",
    "       'data_motor_1_temperature_diff', 'data_motor_1_voltage_diff','data_motor_1_position_diff', \n",
    "       'data_motor_2_position','data_motor_2_temperature', 'data_motor_2_voltage', \n",
    "       'data_motor_2_temperature_diff', 'data_motor_2_voltage_diff', 'data_motor_2_position_diff',\n",
    "       'data_motor_3_position', 'data_motor_3_temperature','data_motor_3_voltage',\n",
    "       'data_motor_3_temperature_diff', 'data_motor_3_voltage_diff','data_motor_3_position_diff', \n",
    "       'data_motor_4_position','data_motor_4_temperature', 'data_motor_4_voltage',\n",
    "       'data_motor_4_temperature_diff', 'data_motor_4_voltage_diff', 'data_motor_4_position_diff',\n",
    "       'data_motor_5_position', 'data_motor_5_temperature','data_motor_5_voltage',\n",
    "       'data_motor_5_temperature_diff', 'data_motor_5_voltage_diff','data_motor_5_position_diff', \n",
    "       'data_motor_6_position', 'data_motor_6_temperature', 'data_motor_6_voltage',\n",
    "       'data_motor_6_temperature_diff', 'data_motor_6_voltage_diff', 'data_motor_6_position_diff']\n",
    "\n",
    "feature_list_all_2 = ['time','data_motor_1_position', 'data_motor_1_temperature',\n",
    "                'data_motor_2_position', 'data_motor_2_temperature',\n",
    "                'data_motor_3_position',\n",
    "                'data_motor_4_temperature',\n",
    "                'data_motor_5_position', 'data_motor_5_temperature', \n",
    "                'data_motor_6_position', 'data_motor_6_temperature', 'data_motor_6_voltage','data_motor_6_temperature_diff', 'data_motor_6_voltage_diff', 'data_motor_6_position_diff']\n",
    "\n",
    "feature_list_all_3 = ['time','data_motor_1_position', 'data_motor_1_temperature',\n",
    "                'data_motor_2_position', 'data_motor_2_temperature',\n",
    "                'data_motor_3_position','data_motor_3_temperature',\n",
    "                'data_motor_4_temperature','data_motor_4_position',\n",
    "                'data_motor_5_position', 'data_motor_5_temperature', 'data_motor_6_temperature', 'data_motor_6_voltage']\n",
    "\n",
<<<<<<< HEAD
    "selected_features= ['time','data_motor_1_position',  'data_motor_1_temperature', \n",
    "                    'data_motor_2_position',  \n",
    "                    'data_motor_3_position',  \n",
    "                    'data_motor_4_position', 'data_motor_4_temperature',\n",
    "                    'data_motor_5_position', 'data_motor_5_temperature',\n",
    "                    'data_motor_6_position', 'data_motor_6_temperature', \n",
    "                    'data_motor_1_voltage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor 6"
=======
    "feature_list_all_5 = ['time',\n",
    "                 'data_motor_2_position', \n",
    "                'data_motor_3_position', \n",
    "                'data_motor_4_position', 'data_motor_3_temperature',\n",
    "                  'data_motor_6_temperature']\n",
    "\n",
    "feature_list_all_6 = ['time',\n",
    "                 'data_motor_2_position', \n",
    "                'data_motor_3_position', \n",
    "                'data_motor_4_position', 'data_motor_3_temperature',\n",
    "                  'data_motor_6_temperature']\n",
    "\n",
    "# Test data.\n",
    "test_id = [\n",
    "    '20240325_155003',\n",
    "    '20240425_093699',\n",
    "    '20240425_094425',\n",
    "    '20240426_140055',\n",
    "    '20240503_163963',\n",
    "    '20240503_164675',\n",
    "    '20240503_165189'\n",
    "]\n",
    "df_test = df_data[df_data['test_condition'].isin(test_id)]"
>>>>>>> 22957a468786236d4518d6903bc3a500f6daa815
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 56,
>>>>>>> 22957a468786236d4518d6903bc3a500f6daa815
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, steps,param_grid, features, window_size, sample_step, prediction_lead_time, threshold, abnormal_limit, motor_idx, n_fold=7):\n",
    "    print(f'Running model: {name}')\n",
    "    \n",
    "    x_tr_org, y_temp_tr_org = extract_selected_feature(df_data=df_tr, feature_list=features, motor_idx=motor_idx, mdl_type='reg')\n",
    "    \n",
    "    x_tr, y_temp_tr = prepare_sliding_window(df_x=x_tr_org, y=y_temp_tr_org, window_size=window_size, sample_step=sample_step, prediction_lead_time=prediction_lead_time, mdl_type='reg')\n",
    "    \n",
    "    pipeline = Pipeline(steps)\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=n_fold, scoring='f1', n_jobs=-1)\n",
    "    \n",
    "    grid_search.fit(x_tr, y_temp_tr)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    #mdl = Pipeline(steps).fit(x_tr, y_temp_tr)\n",
    "    \n",
    "    # Define the fault detector\n",
    "    detector_reg = FaultDetectReg(reg_mdl=best_model, threshold=threshold, abnormal_limit=abnormal_limit, window_size=window_size, sample_step=sample_step, pred_lead_time=prediction_lead_time)\n",
    "    \n",
    "    # # Run cross validation\n",
    "    n_fold = 7\n",
<<<<<<< HEAD
    "    _, y_label_test_org = extract_selected_feature(df_data=df_test, feature_list=features, motor_idx=6, mdl_type='clf')\n",
    "    \n",
    "    # test data\n",
    "    x_test_org, y_temp_test_org = extract_selected_feature(df_data=df_test, feature_list=features, motor_idx=6, mdl_type='reg')\n",
=======
    "    _, y_label_test_org = extract_selected_feature(df_data=df_test, feature_list=features, motor_idx=motor_idx, mdl_type='clf')\n",
    "    x_test_org, y_temp_test_org = extract_selected_feature(df_data=df_test, feature_list=features, motor_idx=motor_idx, mdl_type='reg')\n",
>>>>>>> 22957a468786236d4518d6903bc3a500f6daa815
    "    \n",
    "    print(f'Best parameters for {name}: {grid_search.best_params_}')\n",
    "    \n",
    "    motor_idx = motor_idx\n",
    "    print(f'Model for motor {motor_idx}:')\n",
    "    \n",
    "    # Run cross validation.\n",
    "    df_perf = detector_reg.run_cross_val(df_x=x_test_org, y_label=y_label_test_org, y_response=y_temp_test_org, \n",
    "                                        n_fold=n_fold,single_run_result=False)\n",
    "    \n",
    "    print(f'{name} performance:\\n{df_perf}\\n')\n",
    "    print('Mean performance metric and standard error:')\n",
    "    for metric, error in zip(df_perf.mean(), df_perf.std()):\n",
    "        print(f'{metric:.4f} +- {error:.4f}')\n",
    "    print('\\n')\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault detection based on regression model "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 57,
>>>>>>> 22957a468786236d4518d6903bc3a500f6daa815
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: Linear Regression\n"
     ]
    },
    {
<<<<<<< HEAD
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:04<00:00,  4.83s/it]\n",
      "100%|| 1/1 [00:00<00:00, 13.06it/s]\n",
      "100%|| 1/1 [00:00<00:00,  4.28it/s]\n",
      "100%|| 1/1 [00:00<00:00,  1.88it/s]\n",
      "100%|| 1/1 [00:00<00:00,  1.89it/s]\n",
      "100%|| 1/1 [00:00<00:00,  4.02it/s]\n",
      "100%|| 1/1 [00:00<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression performance:\n",
      "   Accuracy  Precision    Recall  F1 score\n",
      "0  0.883870   0.416274  0.570275  0.481254\n",
      "1  0.787879   0.781250  0.914634  0.842697\n",
      "2  0.885033   0.895833  0.666667  0.764444\n",
      "3  0.971429   1.000000  0.571429  0.727273\n",
      "4  0.894286   0.847059  0.829971  0.838428\n",
      "5  0.838000   0.958621  0.868750  0.911475\n",
      "6  0.525074   0.319079  0.457547  0.375969\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "0.8265 +- 0.1442\n",
      "0.7454 +- 0.2691\n",
      "0.6970 +- 0.1754\n",
      "0.7059 +- 0.2007\n",
      "\n",
      "\n",
      "Running model: Ridge Regression\n",
      "Best parameters for Ridge Regression: {'regressor__alpha': 1e-05}\n",
      "Model for motor 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:05<00:00,  5.14s/it]\n",
      "100%|| 1/1 [00:00<00:00, 11.68it/s]\n",
      "100%|| 1/1 [00:00<00:00,  3.59it/s]\n",
      "100%|| 1/1 [00:00<00:00,  1.83it/s]\n",
      "100%|| 1/1 [00:00<00:00,  1.77it/s]\n",
      "100%|| 1/1 [00:00<00:00,  3.99it/s]\n",
      "100%|| 1/1 [00:00<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression performance:\n",
      "   Accuracy  Precision    Recall  F1 score\n",
      "0  0.884786   0.419240  0.570275  0.483231\n",
      "1  0.787879   0.781250  0.914634  0.842697\n",
      "2  0.885033   0.895833  0.666667  0.764444\n",
      "3  0.971429   1.000000  0.571429  0.727273\n",
      "4  0.895238   0.849558  0.829971  0.839650\n",
      "5  0.838000   0.958621  0.868750  0.911475\n",
      "6  0.525074   0.319079  0.457547  0.375969\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "0.8268 +- 0.1444\n",
      "0.7462 +- 0.2687\n",
      "0.6970 +- 0.1754\n",
      "0.7064 +- 0.2005\n",
      "\n",
      "\n",
      "Running model: Lasso Regression\n",
      "Best parameters for Lasso Regression: {'regressor__alpha': 1e-05}\n",
      "Model for motor 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:04<00:00,  4.98s/it]\n",
      "100%|| 1/1 [00:00<00:00, 14.94it/s]\n",
      "100%|| 1/1 [00:00<00:00,  4.28it/s]\n",
      "100%|| 1/1 [00:00<00:00,  1.72it/s]\n",
      "100%|| 1/1 [00:00<00:00,  1.58it/s]\n",
      "100%|| 1/1 [00:00<00:00,  3.54it/s]\n",
      "100%|| 1/1 [00:00<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression performance:\n",
      "   Accuracy  Precision    Recall  F1 score\n",
      "0  0.889669   0.432468  0.537964  0.479482\n",
      "1  0.780303   0.778947  0.902439  0.836158\n",
      "2  0.882863   0.894737  0.658915  0.758929\n",
      "3  0.967196   0.944444  0.539683  0.686869\n",
      "4  0.871429   0.789617  0.832853  0.810659\n",
      "5  0.828000   0.960280  0.856250  0.905286\n",
      "6  0.532448   0.324415  0.457547  0.379648\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "0.8217 +- 0.1399\n",
      "0.7321 +- 0.2533\n",
      "0.6837 +- 0.1796\n",
      "0.6939 +- 0.1947\n",
      "\n",
      "\n",
      "Running model: ElasticNet Regression\n",
      "Best parameters for ElasticNet Regression: {'regressor__alpha': 1e-05, 'regressor__l1_ratio': 1e-05}\n",
      "Model for motor 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:04<00:00,  4.77s/it]\n",
      "100%|| 1/1 [00:00<00:00, 11.96it/s]\n",
      "100%|| 1/1 [00:00<00:00,  3.75it/s]\n",
      "100%|| 1/1 [00:00<00:00,  1.94it/s]\n",
      "100%|| 1/1 [00:00<00:00,  1.71it/s]\n",
      "100%|| 1/1 [00:00<00:00,  3.75it/s]\n",
      "100%|| 1/1 [00:00<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet Regression performance:\n",
      "   Accuracy  Precision    Recall  F1 score\n",
      "0  0.888753   0.429124  0.537964  0.477419\n",
      "1  0.780303   0.778947  0.902439  0.836158\n",
      "2  0.882863   0.894737  0.658915  0.758929\n",
      "3  0.967196   0.944444  0.539683  0.686869\n",
      "4  0.871429   0.789617  0.832853  0.810659\n",
      "5  0.828000   0.960280  0.856250  0.905286\n",
      "6  0.532448   0.324415  0.457547  0.379648\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "0.8216 +- 0.1398\n",
      "0.7317 +- 0.2540\n",
      "0.6837 +- 0.1796\n",
      "0.6936 +- 0.1951\n",
      "\n",
      "\n",
      "Running model: Decision Tree Regression\n",
      "Best parameters for Decision Tree Regression: {'regressor__max_depth': 2, 'regressor__min_samples_split': 2}\n",
      "Model for motor 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:04<00:00,  4.08s/it]\n",
      "100%|| 1/1 [00:00<00:00, 19.98it/s]\n",
      "100%|| 1/1 [00:00<00:00,  4.62it/s]\n",
      "100%|| 1/1 [00:00<00:00,  2.14it/s]\n",
      "100%|| 1/1 [00:00<00:00,  1.77it/s]\n",
      "100%|| 1/1 [00:00<00:00,  3.72it/s]\n",
      "100%|| 1/1 [00:00<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regression performance:\n",
      "   Accuracy  Precision    Recall  F1 score\n",
      "0  0.524035   0.165597  1.000000  0.284140\n",
      "1  0.765152   0.774194  0.878049  0.822857\n",
      "2  0.882863   1.000000  0.581395  0.735294\n",
      "3  0.961905   1.000000  0.428571  0.600000\n",
      "4  0.840000   0.793443  0.697406  0.742331\n",
      "5  0.746000   0.951407  0.775000  0.854191\n",
      "6  0.328909   0.284192  0.754717  0.412903\n",
      "\n",
      "Mean performance metric and standard error:\n",
      "0.7213 +- 0.2212\n",
      "0.7098 +- 0.3453\n",
      "0.7307 +- 0.1877\n",
      "0.6360 +- 0.2155\n",
      "\n",
      "\n",
      "| Model   | Accuracy | Precision | Recall | F1   |\n",
      "|---------|----------|-----------|--------|------|\n",
      "| Linear Regression | 0.83 | 0.75 | 0.70 | 0.71 |\n",
      "| Ridge Regression | 0.83 | 0.75 | 0.70 | 0.71 |\n",
      "| Lasso Regression | 0.82 | 0.73 | 0.68 | 0.69 |\n",
      "| ElasticNet Regression | 0.82 | 0.73 | 0.68 | 0.69 |\n",
      "| Decision Tree Regression | 0.72 | 0.71 | 0.73 | 0.64 |\n",
      "\n"
=======
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=7 greater than the number of samples: n_samples=0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model_steps \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m     39\u001b[0m     param_grid \u001b[38;5;241m=\u001b[39m param_grids[model_name]\n\u001b[1;32m---> 40\u001b[0m     df_perf \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_list_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_lead_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabnormal_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmotor_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     mean_perf \u001b[38;5;241m=\u001b[39m df_perf\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     42\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((model_name, mean_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], mean_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m], mean_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m], mean_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 score\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[56], line 12\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(name, steps, param_grid, features, window_size, sample_step, prediction_lead_time, threshold, abnormal_limit, motor_idx, n_fold)\u001b[0m\n\u001b[0;32m      9\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps)\n\u001b[0;32m     10\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39mn_fold, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_temp_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#mdl = Pipeline(steps).fit(x_tr, y_temp_tr)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Define the fault detector\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:833\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[0;32m    821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[1;32m--> 833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_split.py:345\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    343\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    346\u001b[0m         (\n\u001b[0;32m    347\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    349\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[0;32m    350\u001b[0m     )\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups):\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of splits n_splits=7 greater than the number of samples: n_samples=0."
>>>>>>> 22957a468786236d4518d6903bc3a500f6daa815
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Motor Id\n",
    "motor_idx = 5\n",
    "\n",
    "# Enrich the features based on the sliding window.\n",
    "window_size = 80\n",
    "sample_step = 60\n",
    "prediction_lead_time = 5 \n",
    "threshold = .5\n",
    "abnormal_limit = 3\n",
    "\n",
    "#param_grids = {\n",
    "    #'Linear Regression': [{}],  # Note: GridSearchCV requires at least one parameter grid\n",
    "    #'Ridge Regression': [{'regressor__alpha': np.logspace(-7, 1, num=100)}],# always choose the lowest value\n",
    "    #'Lasso Regression': [{'regressor__alpha': np.logspace(-7, 1, num=100)}],# always choose the lowest value\n",
    "    #'ElasticNet Regression': [{'regressor__alpha': np.logspace(-7, 1, num=100), 'regressor__l1_ratio': np.logspace(-7, 1, num=100)}], # always choose the lowest value\n",
    "    #'Decision Tree Regression': [{'regressor__max_depth': [2,3,4], 'regressor__min_samples_split': [2,3,4]}] # always choose the lowest value\n",
    "#}\n",
    "\n",
    "#fix parameters to run faster, since we already know the best chosen parameters\n",
    "\n",
    "param_grids = {\n",
    "    'Linear Regression': [{}],  # Note: GridSearchCV requires at least one parameter grid\n",
    "    'Ridge Regression': [{'regressor__alpha': [0.00001]}],# always choose the lowest value\n",
    "    'Lasso Regression': [{'regressor__alpha': [0.00001]}],# always choose the lowest value\n",
    "    'ElasticNet Regression': [{'regressor__alpha': [0.00001], 'regressor__l1_ratio': [0.00001]}], # always choose the lowest value\n",
    "    'Decision Tree Regression': [{'regressor__max_depth': [2], 'regressor__min_samples_split': [2]}] # always choose the lowest value\n",
    "}\n",
    "\n",
    "results = []\n",
    "models = [\n",
    "    ('Linear Regression', [('Normalizer', MinMaxScaler()), ('regressor', LinearRegression())]),\n",
    "    ('Ridge Regression', [('Normalizer', MinMaxScaler()), ('regressor', Ridge())]),\n",
    "    ('Lasso Regression', [('Normalizer', MinMaxScaler()), ('regressor', Lasso())]),\n",
    "    ('ElasticNet Regression', [('Normalizer', MinMaxScaler()), ('regressor', ElasticNet())]),\n",
    "    ('Decision Tree Regression', [('regressor', DecisionTreeRegressor())])\n",
    "]\n",
    "\n",
    "for model_name, model_steps in models:\n",
    "    param_grid = param_grids[model_name]\n",
    "    df_perf = evaluate_model(model_name, model_steps,param_grid, feature_list_all, window_size, sample_step, prediction_lead_time, threshold, abnormal_limit, motor_idx)\n",
    "    mean_perf = df_perf.mean()\n",
    "    results.append((model_name, mean_perf['Accuracy'], mean_perf['Precision'], mean_perf['Recall'], mean_perf['F1 score']))\n",
    "\n",
    "table_md = \"| Model   | Accuracy | Precision | Recall | F1   |\\n\"\n",
    "table_md += \"|---------|----------|-----------|--------|------|\\n\"\n",
    "for result in results:\n",
    "    model_name, accuracy, precision, recall, f1 = result\n",
    "    table_md += f\"| {model_name} | {accuracy:.2f} | {precision:.2f} | {recall:.2f} | {f1:.2f} |\\n\"\n",
    "\n",
    "print(table_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Problem : we have to return the y_pred in the run_cross_val in utility.py"
=======
    "Results Regression motor 5 with features from motor 6.\n",
    "\n",
    "| Model   | Accuracy | Precision | Recall | F1   |\n",
    "|---------|----------|-----------|--------|------|\n",
    "| Linear Regression | 0.91 | 0.52 | 0.68 | 0.54 |\n",
    "| Ridge Regression | 0.91 | 0.52 | 0.68 | 0.54 |\n",
    "| Lasso Regression | 0.91 | 0.66 | 0.83 | 0.68 |\n",
    "| ElasticNet Regression | 0.91 | 0.66 | 0.83 | 0.68 |\n",
    "| Decision Tree Regression | 0.73 | 0.53 | 0.58 | 0.54 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results Regression motor 5 with all features. (moins bien)\n",
    "\n",
    "| Model   | Accuracy | Precision | Recall | F1   |\n",
    "|---------|----------|-----------|--------|------|\n",
    "| Linear Regression | 0.95 | 0.50 | 0.56 | 0.52 |\n",
    "| Ridge Regression | 0.95 | 0.50 | 0.56 | 0.52 |\n",
    "| Lasso Regression | 0.95 | 0.50 | 0.56 | 0.52 |\n",
    "| ElasticNet Regression | 0.95 | 0.50 | 0.56 | 0.52 |\n",
    "| Decision Tree Regression | 0.73 | 0.53 | 0.58 | 0.54 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the results\n",
    "\n",
    "Please add a table in the end, summarying the results from all the models (including the unsupervised learning models). Please write a few texts to explain what is the best model you got (including key parameters like threshold, window_size, sample_step, prediction_lead_time, etc), its performance, and how could you further improve it.\n",
    "\n",
    "| Model   | Accuracy | Precision | Recall | F1   |\n",
    "|---------|----------|-----------|--------|------|\n",
    "| Linear Regression | 0.83 | 0.75 | 0.70 | 0.71 |\n",
    "| Ridge Regression | 0.83 | 0.75 | 0.70 | 0.71 |\n",
    "| Lasso Regression | 0.82 | 0.73 | 0.68 | 0.69 |\n",
    "| ElasticNet Regression | 0.82 | 0.73 | 0.68 | 0.69 |\n",
    "| Decision Tree Regression | 0.72 | 0.71 | 0.73 | 0.64 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Best model : Linear Regression\n",
    "\n",
    "- window_size = 80\n",
    "\n",
    "- sample_step = 60\n",
    "\n",
    "- prediction_lead_time = 5 \n",
    "\n",
    "- threshold = .5\n",
    "\n",
    "- abnormal_limit = 2"
>>>>>>> 22957a468786236d4518d6903bc3a500f6daa815
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
