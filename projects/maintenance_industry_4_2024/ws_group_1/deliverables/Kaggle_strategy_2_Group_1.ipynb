{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle project 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import make_scorer,accuracy_score ,precision_score, recall_score\n",
    "from sklearn.metrics import f1_score,confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from functions import *\n",
    "\n",
    "from utility import read_all_csvs_one_test\n",
    "from utility import read_all_test_data_from_path\n",
    "from utility import run_cv_one_motor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_int = 20\n",
    "\n",
    "# Subfunction for data preprocessing.\n",
    "def pre_processing(df: pd.DataFrame):\n",
    "    ''' ### Description\n",
    "    Preprocess the data:\n",
    "    - remove outliers\n",
    "    - add new features about the difference between the current and previous n data point.\n",
    "    '''\n",
    "    \n",
    "    def remove_outliers(df: pd.DataFrame):\n",
    "        ''' # Description\n",
    "        Remove outliers from the dataframe based on defined valid ranges. \n",
    "        Define a valid range of temperature and voltage. \n",
    "        Use ffil function to replace the invalid measurement with the previous value.\n",
    "        '''\n",
    "        df['temperature'] = df['temperature'].where(df['temperature'] <= 100, np.nan)\n",
    "        df['temperature'] = df['temperature'].where(df['temperature'] >= 0, np.nan)\n",
    "        df['temperature'] = df['temperature'].ffill()        \n",
    "\n",
    "        df['voltage'] = df['voltage'].where(df['voltage'] >= 6000, np.nan)\n",
    "        df['voltage'] = df['voltage'].where(df['voltage'] <= 9000, np.nan)\n",
    "        df['voltage'] = df['voltage'].ffill()        \n",
    "\n",
    "        df['position'] = df['position'].where(df['position'] >= 0, np.nan)\n",
    "        df['position'] = df['position'].where(df['position'] <= 1000, np.nan)\n",
    "        df['position'] = df['position'].ffill()\n",
    "\n",
    "\n",
    "    def cal_diff(df: pd.DataFrame, n_int: int):\n",
    "        ''' # Description\n",
    "        Calculate the difference between the current and previous n data point.\n",
    "        '''\n",
    "        # Tranform the features relative to the first data point.\n",
    "        df['temperature'] = df['temperature'] - df['temperature'].iloc[0]\n",
    "        df['voltage'] = df['voltage'] - df['voltage'].iloc[0]\n",
    "        df['position'] = df['position'] - df['position'].iloc[0]\n",
    "\n",
    "        # Calculate the difference between the current and previous n data point.\n",
    "        df['temperature_diff'] = df['temperature'].diff(n_int)\n",
    "        df['voltage_diff'] = df['voltage'].diff(n_int)\n",
    "        df['position_diff'] = df['position'].diff(n_int)   \n",
    "\n",
    "    # Start processing.\n",
    "    remove_outliers(df)\n",
    "    #cal_diff(df, n_int)\n",
    "    \n",
    "# Read all the training dataset.\n",
    "base_dictionary = '../../dataset/training_data/'\n",
    "df_data = read_all_test_data_from_path(base_dictionary, pre_processing, is_plot=False)\n",
    "\n",
    "# Smooth the data.\n",
    "\n",
    "smoothed_data = df_data.copy(deep=True)\n",
    "smoothed_data.drop(columns=['time','test_condition'], inplace=True)\n",
    "\n",
    "for i in range(1,7):\n",
    "    smoothed_data[f'data_motor_{i}_voltage'] = smooth_data_moving_average(smoothed_data[f'data_motor_{i}_voltage'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntest_id = [\\n    '20240527_094865',\\n    '20240527_100759',\\n    '20240527_101627',\\n    '20240527_102436',\\n    '20240527_102919',\\n    '20240527_103311',\\n    '20240527_103690',\\n    '20240527_104247'\\n]\\ndf_test = df_data[df_data['test_condition'].isin(test_id)]\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "test_id = [\n",
    "    '20240527_094865',\n",
    "    '20240527_100759',\n",
    "    '20240527_101627',\n",
    "    '20240527_102436',\n",
    "    '20240527_102919',\n",
    "    '20240527_103311',\n",
    "    '20240527_103690',\n",
    "    '20240527_104247'\n",
    "]\n",
    "df_test = df_data[df_data['test_condition'].isin(test_id)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dictionary = '../../dataset/testing_data/'\n",
    "# Get all the folders in the base_dictionary\n",
    "path_list = os.listdir(base_dictionary)\n",
    "# Only keep the folders, not the excel file.\n",
    "path_list_sorted = sorted(path_list)\n",
    "path_list = path_list_sorted[:-1]\n",
    "\n",
    "# Read the data.\n",
    "df_test = pd.DataFrame()\n",
    "for tmp_path in path_list:\n",
    "    path = base_dictionary + tmp_path\n",
    "    #tmp_df = read_all_csvs_one_test(path, tmp_path, pre_processing)\n",
    "    \n",
    "    ### ------------read_all_csvs_one_test --------------\n",
    "    \n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = [file for file in os.listdir(path) if file.endswith('.csv')]\n",
    "\n",
    "    # Create an empty DataFrame to store the combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over the CSV files in the folder\n",
    "    for file in csv_files:\n",
    "        # Construct the full path to each CSV file\n",
    "        file_path = os.path.join(path, file)\n",
    "\n",
    "        # Read each CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Drop the time. Will add later.\n",
    "        df = df.drop(labels=df.columns[0], axis=1)\n",
    "\n",
    "        # Apply the pre-processing.\n",
    "        if pre_processing:\n",
    "            pre_processing(df)\n",
    "\n",
    "        # Extract the file name (excluding the extension) to use as a prefix\n",
    "        file_name = os.path.splitext(file)[0]\n",
    "\n",
    "        # Add a prefix to each column based on the file name\n",
    "        df = df.add_prefix(f'{file_name}_')\n",
    "\n",
    "        # Concatenate the current DataFrame with the combined DataFrame\n",
    "        combined_df = pd.concat([combined_df, df], axis=1)\n",
    "\n",
    "    # Add time and test condition\n",
    "    df = pd.read_csv(file_path)\n",
    "    combined_df = pd.concat([df['time'], combined_df], axis=1)\n",
    "\n",
    "    # Calculate the time difference since the first row\n",
    "    time_since_first_row = combined_df['time'] - combined_df['time'].iloc[0]\n",
    "    # Replace the 'time' column with the time difference\n",
    "    combined_df['time'] = time_since_first_row\n",
    "\n",
    "    combined_df.loc[:, 'test_condition'] = tmp_path\n",
    "\n",
    "    combined_df.drop(columns=label_columns, inplace= True)\n",
    "    \n",
    "    # Drop the NaN values, which represents the first n data points in the original dataframe.\n",
    "    combined_df.dropna(inplace=True)\n",
    "\n",
    "    tmp_df = combined_df\n",
    "    \n",
    "    ### --------------------------------------------\n",
    "    \n",
    "    df_test = pd.concat([df_test, tmp_df])\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "# Read the test conditions\n",
    "df_test_conditions = pd.read_excel(base_dictionary+'Test conditions.xlsx')\n",
    "\n",
    "# Smooth the data.\n",
    "\n",
    "df_test.drop(columns=['time','test_condition'], inplace=True)\n",
    "\n",
    "for i in range(1,7):\n",
    "    df_test[f'data_motor_{i}_voltage'] = smooth_data_moving_average(df_data[f'data_motor_{i}_voltage'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features\n",
    "\n",
    "drop_list1_label1 = ['data_motor_2_voltage', 'data_motor_3_voltage', 'data_motor_4_voltage', 'data_motor_5_voltage', 'data_motor_6_voltage', 'data_motor_6_position']\n",
    "\n",
    "drop_list2_label1 = ['data_motor_2_voltage', 'data_motor_3_voltage', 'data_motor_4_voltage', 'data_motor_5_voltage', 'data_motor_6_voltage', 'data_motor_6_position','data_motor_2_position','data_motor_3_position','data_motor_4_position','data_motor_5_position','data_motor_2_temperature','data_motor_3_temperature','data_motor_4_temperature','data_motor_6_temperature']\n",
    "\n",
    "#drop_list1_label1 = ['data_motor_1_voltage','data_motor_2_voltage', 'data_motor_3_voltage', 'data_motor_4_voltage', 'data_motor_5_voltage', 'data_motor_6_voltage', 'data_motor_6_position']\n",
    "\n",
    "label_columns = ['data_motor_1_label', 'data_motor_2_label', 'data_motor_3_label', 'data_motor_4_label', 'data_motor_5_label', 'data_motor_6_label']\n",
    "\n",
    "X = smoothed_data.drop(columns=label_columns+drop_list1_label1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation and undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_and_undersampling(X,y):\n",
    "    \n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train_unsampled, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_unsampled, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        ## undersampling \n",
    "        \n",
    "        # Class count\n",
    "        count_class_0, count_class_1 = y_train_unsampled.value_counts()\n",
    "\n",
    "        # Separate majority and minority classes\n",
    "        data_Normal = X_train_unsampled[y_train_unsampled == 0]\n",
    "        data_Failure = X_train_unsampled[y_train_unsampled == 1]\n",
    "\n",
    "        # Undersample majority class\n",
    "        data_Normal_under = data_Normal.sample(count_class_1)\n",
    "        data_under = pd.concat([data_Normal_under, data_Failure], axis=0)\n",
    "        \n",
    "        undersampled_indices = data_under.index\n",
    "\n",
    "        X_train = X_train_unsampled.loc[undersampled_indices]\n",
    "        y_train = y_train_unsampled.loc[undersampled_indices]\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_motors(label):\n",
    "    X = smoothed_data.drop(columns=label_columns+drop_list1_label1)\n",
    "    y = smoothed_data[label]\n",
    "    \n",
    "    X_train, y_train, X_test , y_test = cross_validation_and_undersampling(X,y)\n",
    "    \n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(class_weight='balanced'),\n",
    "        'Decision Tree': DecisionTreeClassifier(class_weight='balanced'),\n",
    "        'Random Forest': RandomForestClassifier(class_weight='balanced'),\n",
    "        'Support Vector Machine': SVC(class_weight='balanced'),\n",
    "        'Gradient Boosting': GradientBoostingClassifier()\n",
    "    }\n",
    "\n",
    "    # Dictionary to store model performance metrics\n",
    "    model_metrics = {}\n",
    "\n",
    "    # Define hyperparameter grids\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "        'Decision Tree': {'max_depth': [None, 10, 20]},\n",
    "        'Random Forest': {'n_estimators': [50, 100, 200]},\n",
    "        'Support Vector Machine': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']},\n",
    "        'Gradient Boosting': {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.5, 1.0]}\n",
    "    }\n",
    "\n",
    "    model_predictions = {}\n",
    "\n",
    "    # Perform cross-validation, hyperparameter tuning, and evaluation\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        grid_search = GridSearchCV(model, param_grids[model_name], cv=5, scoring='f1')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        model_predictions[f'y_pred_{model_name.replace(\" \", \"_\")}'] = y_pred\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        # Store metrics in the dictionary\n",
    "        model_metrics[model_name] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1\n",
    "        }\n",
    "        \n",
    "    # Update the summary table with the model performance metrics\n",
    "    summary_table = \"| Model                    | Accuracy | Precision | Recall | F1    |\\n\"\n",
    "    summary_table += \"|--------------------------|----------|-----------|--------|-------|\\n\"\n",
    "    for model_name, metrics in model_metrics.items():\n",
    "        summary_table += f\"| {model_name:25} | {metrics['Accuracy']*100:.2f}%   | {metrics['Precision']*100:.2f}%   | {metrics['Recall']*100:.2f}%  | {metrics['F1']*100:.2f}% |\\n\"\n",
    "\n",
    "    print(summary_table)        \n",
    "    \n",
    "    return model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = smoothed_data.drop(columns=label_columns+drop_list1_label1).values\n",
    "X_test = df_test.drop(columns=drop_list1_label1).values\n",
    "\n",
    "def run_all_motors_validation(label):\n",
    "    y = smoothed_data[label]\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(class_weight='balanced'),\n",
    "        'Decision Tree': DecisionTreeClassifier(class_weight='balanced'),\n",
    "        'Random Forest': RandomForestClassifier(class_weight='balanced'),\n",
    "        'Support Vector Machine': SVC(class_weight='balanced'),\n",
    "        'Gradient Boosting': GradientBoostingClassifier()\n",
    "    }\n",
    "\n",
    "    # Define hyperparameter grids\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {'C': [0.1]},\n",
    "        'Decision Tree': {'max_depth': [10]},\n",
    "        'Random Forest': {'n_estimators': [50]},\n",
    "        'Support Vector Machine': {'C': [0.1], 'gamma': ['scale', 'auto']},\n",
    "        'Gradient Boosting': {'n_estimators': [50], 'learning_rate': [0.5]}\n",
    "    }\n",
    "\n",
    "    model_predictions = {}\n",
    "\n",
    "    # Perform cross-validation, hyperparameter tuning, and evaluation\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', MinMaxScaler()), # Step 1 : Normalization\n",
    "            ('model', model)\n",
    "        ])\n",
    "        param_grid = {f'model__{key}': value for key, value in param_grids[model_name].items()}\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1')\n",
    "        #grid_search = GridSearchCV(model, param_grids[model_name], cv=5, scoring='f1')\n",
    "        grid_search.fit(X, y)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        model_predictions[f'y_pred_{model_name.replace(\" \", \"_\")}'] = y_pred    \n",
    "    \n",
    "    return model_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                    | Accuracy | Precision | Recall | F1    |\n",
      "|--------------------------|----------|-----------|--------|-------|\n",
      "| Logistic Regression       | 82.68%   | 17.08%   | 97.87%  | 29.08% |\n",
      "| Decision Tree             | 99.18%   | 82.25%   | 98.58%  | 89.68% |\n",
      "| Random Forest             | 99.42%   | 86.69%   | 99.29%  | 92.56% |\n",
      "| Support Vector Machine    | 95.10%   | 42.28%   | 96.10%  | 58.72% |\n",
      "| Gradient Boosting         | 99.20%   | 82.35%   | 99.29%  | 90.03% |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_predictions = run_all_motors('data_motor_1_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = run_all_motors_validation('data_motor_1_label')\n",
    "y_pred1 = model_predictions['y_pred_Gradient_Boosting']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                    | Accuracy | Precision | Recall | F1    |\n",
      "|--------------------------|----------|-----------|--------|-------|\n",
      "| Logistic Regression       | 89.33%   | 24.98%   | 96.81%  | 39.71% |\n",
      "| Decision Tree             | 99.23%   | 82.84%   | 99.29%  | 90.32% |\n",
      "| Random Forest             | 99.86%   | 96.89%   | 99.29%  | 98.07% |\n",
      "| Support Vector Machine    | 95.28%   | 43.22%   | 96.10%  | 59.63% |\n",
      "| Gradient Boosting         | 99.68%   | 92.13%   | 99.65%  | 95.74% |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_predictions = run_all_motors('data_motor_2_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = run_all_motors_validation('data_motor_2_label')\n",
    "y_pred2 = model_predictions['y_pred_Gradient_Boosting']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                    | Accuracy | Precision | Recall | F1    |\n",
      "|--------------------------|----------|-----------|--------|-------|\n",
      "| Logistic Regression       | 85.34%   | 19.50%   | 97.16%  | 32.48% |\n",
      "| Decision Tree             | 99.46%   | 87.74%   | 98.94%  | 93.00% |\n",
      "| Random Forest             | 99.88%   | 97.56%   | 99.29%  | 98.42% |\n",
      "| Support Vector Machine    | 95.46%   | 44.21%   | 96.10%  | 60.56% |\n",
      "| Gradient Boosting         | 99.83%   | 96.22%   | 99.29%  | 97.73% |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_predictions = run_all_motors('data_motor_3_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = run_all_motors_validation('data_motor_3_label')\n",
    "y_pred3 = model_predictions['y_pred_Gradient_Boosting']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                    | Accuracy | Precision | Recall | F1    |\n",
      "|--------------------------|----------|-----------|--------|-------|\n",
      "| Logistic Regression       | 85.56%   | 19.57%   | 95.74%  | 32.49% |\n",
      "| Decision Tree             | 99.21%   | 82.79%   | 98.94%  | 90.15% |\n",
      "| Random Forest             | 99.90%   | 97.90%   | 99.29%  | 98.59% |\n",
      "| Support Vector Machine    | 95.64%   | 45.21%   | 95.39%  | 61.35% |\n",
      "| Gradient Boosting         | 99.88%   | 97.56%   | 99.29%  | 98.42% |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_predictions = run_all_motors('data_motor_4_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = run_all_motors_validation('data_motor_4_label')\n",
    "y_pred4 = model_predictions['y_pred_Gradient_Boosting']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                    | Accuracy | Precision | Recall | F1    |\n",
      "|--------------------------|----------|-----------|--------|-------|\n",
      "| Logistic Regression       | 89.52%   | 25.46%   | 97.87%  | 40.41% |\n",
      "| Decision Tree             | 99.11%   | 81.05%   | 98.58%  | 88.96% |\n",
      "| Random Forest             | 99.83%   | 96.22%   | 99.29%  | 97.73% |\n",
      "| Support Vector Machine    | 95.17%   | 42.70%   | 96.45%  | 59.19% |\n",
      "| Gradient Boosting         | 99.34%   | 84.89%   | 99.65%  | 91.68% |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_predictions = run_all_motors('data_motor_5_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = run_all_motors_validation('data_motor_5_label')\n",
    "y_pred5 = model_predictions['y_pred_Gradient_Boosting']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                    | Accuracy | Precision | Recall | F1    |\n",
      "|--------------------------|----------|-----------|--------|-------|\n",
      "| Logistic Regression       | 83.50%   | 17.66%   | 96.81%  | 29.87% |\n",
      "| Decision Tree             | 99.42%   | 86.69%   | 99.29%  | 92.56% |\n",
      "| Random Forest             | 99.86%   | 97.21%   | 98.94%  | 98.07% |\n",
      "| Support Vector Machine    | 95.83%   | 46.40%   | 96.10%  | 62.59% |\n",
      "| Gradient Boosting         | 99.87%   | 96.90%   | 99.65%  | 98.25% |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_predictions = run_all_motors('data_motor_6_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = run_all_motors_validation('data_motor_6_label')\n",
    "y_pred6 = model_predictions['y_pred_Gradient_Boosting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'idx': range(len(y_pred1)),\n",
    "    'data_motor_1_label': y_pred1,\n",
    "    'data_motor_2_label': y_pred2,\n",
    "    'data_motor_3_label': y_pred3,\n",
    "    'data_motor_4_label': y_pred4,\n",
    "    'data_motor_5_label': y_pred5,\n",
    "    'data_motor_6_label': y_pred6\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('motor_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
